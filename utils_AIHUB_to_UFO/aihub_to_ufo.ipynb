{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "dataset_name = 'aihub_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_path_root = '/opt/ml/utils/aihub_to_ufo/images'\n",
    "original_label_path_root = '/opt/ml/utils/aihub_to_ufo/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_path_root = os.path.join('/opt/ml/input/data/', dataset_name, 'images')\n",
    "new_label_path_root = os.path.join('/opt/ml/input/data/', dataset_name, 'labels_icdar')\n",
    "\n",
    "os.makedirs(new_image_path_root, exist_ok=True)\n",
    "os.makedirs(new_label_path_root, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## aihub to icdar\n",
    "/opt/ml/utils/aihub_to_ufo/images 폴더에 있는 이미지를\n",
    "\n",
    "/opt/ml/input/data/{dataset_name}/images 폴더로 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_image_path in glob(original_image_path_root+'/*'):\n",
    "    original_image_name = os.path.basename(original_image_path)\n",
    "    new_image_path = os.path.join(new_image_path_root, original_image_name)\n",
    "    \n",
    "    ## '/opt/ml/utils/aihub_to_ufo/images' 에 있는 파일 복사. 원본 유지\n",
    "    shutil.copy(original_image_path, new_image_path)\n",
    "    \n",
    "    ## '/opt/ml/utils/aihub_to_ufo/images' 에 있는 파일 옮기고 지우기\n",
    "    # shutil.move(original_image_path, new_image_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/opt/ml/utils/aihub_to_ufo/labels 폴더에 있는 aihub 포맷 레이블을\n",
    "\n",
    "/opt/ml/input/data/{dataset_name}/labels_icdar 폴더로 icdar 포맷으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_label_path in glob(original_label_path_root+'/*'):\n",
    "\n",
    "    with open(original_label_path, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    icdar_format = []\n",
    "    for aihub_label in data['annotations']:\n",
    "        x = aihub_label['bbox'][0]\n",
    "        y = aihub_label['bbox'][1]\n",
    "        width = aihub_label['bbox'][2]\n",
    "        height = aihub_label['bbox'][3]\n",
    "        if aihub_label['text'] == 'xxx':\n",
    "            text = '###'\n",
    "        else:\n",
    "            text = aihub_label['text']\n",
    "        \n",
    "        icdar_format.append(f\"{x},{y},{x+width},{y},{x+width},{y+height},{x},{y+height},Korean,{text}\")\n",
    "        \n",
    "    new_label_name = os.path.basename(original_label_path)\n",
    "    new_label_name = os.path.splitext(new_label_name)[0]\n",
    "    with open(os.path.join(new_label_path_root,f'gt_{new_label_name}.txt'), 'w', encoding='utf-8') as txt:\n",
    "        txt.write('\\n'.join(icdar_format))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## icdar to ufo\n",
    "/opt/ml/input/data/{dataset_name}/labels_icdar 폴더에 있는 icdar 포맷 레이블을\n",
    "\n",
    "/opt/ml/input/data/{dataset_name}/ufo/train.json 로 ufo 포맷으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTENSIONS = {'.gif', '.GIF', '.jpg', '.JPG', '.png', '.PNG', '.jpeg', '.JPEG'}\n",
    "LANGUAGE_MAP = {\n",
    "    'Korean': 'ko',\n",
    "    'Latin': 'en',\n",
    "    'Symbols': None\n",
    "}\n",
    "def get_language_token(x):\n",
    "    return LANGUAGE_MAP.get(x, 'others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLT17Dataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir):\n",
    "        image_paths = set(x for x in glob(os.path.join(image_dir, '*')) if os.path.splitext(x)[1] in\n",
    "                       IMAGE_EXTENSIONS)\n",
    "        label_paths = set(glob(os.path.join(label_dir, '*.txt')))\n",
    "        assert len(image_paths) == len(label_paths)\n",
    "\n",
    "        ## sample_id 예시: 'img_999'\n",
    "        sample_ids, samples_info = list(), dict()\n",
    "        for image_path in image_paths:\n",
    "            sample_id = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "            label_path = os.path.join(label_dir, 'gt_{}.txt'.format(sample_id))\n",
    "            assert label_path in label_paths\n",
    "\n",
    "            words_info, extra_info = self.parse_label_file(label_path)\n",
    "            if 'ko' not in extra_info['languages'] or extra_info['languages'].difference({'ko', 'en'}):\n",
    "                continue\n",
    "\n",
    "            sample_ids.append(sample_id)\n",
    "            samples_info[sample_id] = dict(image_path=image_path, label_path=label_path,\n",
    "                                           words_info=words_info)\n",
    "\n",
    "        self.sample_ids, self.samples_info = sample_ids, samples_info\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.samples_info[self.sample_ids[idx]]\n",
    "\n",
    "        image_fname = os.path.basename(sample_info['image_path'])\n",
    "        image = Image.open(sample_info['image_path'])\n",
    "        img_w, img_h = image.size\n",
    "\n",
    "        license_tag = dict(usability=True, public=True, commercial=True, type='CC-BY-SA',\n",
    "                           holder=None)\n",
    "        sample_info_ufo = dict(img_h=img_h, img_w=img_w, words=sample_info['words_info'], tags=None,\n",
    "                               license_tag=license_tag)\n",
    "\n",
    "        return image_fname, sample_info_ufo\n",
    "\n",
    "    def parse_label_file(self, label_path):\n",
    "        def rearrange_points(points):\n",
    "            # points 내부의 점들의 위치를 체크\n",
    "            # np.linalg.norm(p, ord=1)은 0번 axis 기준으로 sum 하는 것.\n",
    "            # [[1376.0, 0.0], [1600.0, 0.0], [1600.0, 341.0], [1376.0, 341.0]] 같은 게\n",
    "            # [1376.0, 1600.0, 1941.0, 1717.0] 로 변환. \n",
    "            # 좌측 상단이 (0,0) 이니까 가장 처음 오는 point의 x값+y값이 최소여야 한다는 것.\n",
    "            start_idx = np.argmin([np.linalg.norm(p, ord=1) for p in points])\n",
    "            \n",
    "            # 이 때 가장 작은 값이 0번째 point인지를 확인\n",
    "            if start_idx != 0:\n",
    "                # (만약 아니라면) start_idx에 해당하는 point가 맨 앞으로 오게 roll\n",
    "                points = np.roll(points, -start_idx, axis=0).tolist()\n",
    "            return points\n",
    "\n",
    "        with open(label_path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        words_info, languages = dict(), set()\n",
    "        for word_idx, line in enumerate(lines):\n",
    "            items = line.strip().split(',', 9)\n",
    "            language, transcription = items[8], items[9]\n",
    "            points = np.array(items[:8], dtype=np.float32).reshape(4, 2).tolist()\n",
    "            points = rearrange_points(points)\n",
    "\n",
    "            illegibility = transcription == '###'\n",
    "            orientation = 'Horizontal'\n",
    "            language = get_language_token(language)\n",
    "            words_info[word_idx] = dict(\n",
    "                points=points, transcription=transcription, language=[language],\n",
    "                illegibility=illegibility, orientation=orientation, word_tags=None\n",
    "            )\n",
    "            languages.add(language)\n",
    "\n",
    "        return words_info, dict(languages=languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlt_train = MLT17Dataset(new_image_path_root, new_label_path_root)\n",
    "\n",
    "    anno = dict(images=dict())\n",
    "    with tqdm(total=len(mlt_train)) as progress_bar:\n",
    "        for batch in DataLoader(mlt_train, num_workers=32, collate_fn=lambda x: x):\n",
    "            image_fname, sample_info = batch[0]\n",
    "            anno['images'][image_fname] = sample_info\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    ufo_dir = os.path.join(os.path.dirname(new_label_path_root), 'ufo')\n",
    "    os.makedirs(ufo_dir, exist_ok=True)\n",
    "    with open(os.path.join(ufo_dir, 'train.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(anno, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 원본 제거하고싶으면 실행\n",
    "- /opt/ml/utils/aihub_to_ufo/images\n",
    "\n",
    "- /opt/ml/utils/aihub_to_ufo/labels\n",
    "\n",
    "- /opt/ml/input/data/{dataset_name}/labels_icdar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(original_image_path_root)\n",
    "# shutil.rmtree(original_label_path_root)\n",
    "# shutil.rmtree(new_label_path_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
