{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### based on https://github.com/jason9075/opencv-mosaic-data-aug by jason9075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "dataset_0 = 'ICDAR19_MLT'\n",
    "dataset_1 = 'ICDAR19_MLT' \n",
    "dataset_2 = 'ICDAR17_Korean'\n",
    "dataset_3 = 'ICDAR19_Korean'\n",
    "\n",
    "TOTAL = 2000                   # 생성할 이미지 개수\n",
    "OUTPUT_SIZE = (1024, 1024)  # Height, Width\n",
    "SCALE_RANGE = (0.3, 0.7)    # 모자이크 4사분면 나누는 축. 4사진 다 똑같은 크기 하고싶으면 (0.5, 0.5)\n",
    "FILTER_TINY = 0             # 2500면 50px * 50px 보다 작은 bbox는 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR_0 = os.path.join('/opt/ml/input/data/', dataset_0, 'images')\n",
    "IMG_DIR_1 = os.path.join('/opt/ml/input/data/', dataset_1, 'images')\n",
    "IMG_DIR_2 = os.path.join('/opt/ml/input/data/', dataset_2, 'images')\n",
    "IMG_DIR_3 = os.path.join('/opt/ml/input/data/', dataset_3, 'images')\n",
    "\n",
    "LABEL_DIR_0 = os.path.join('/opt/ml/input/data/', dataset_0, 'ufo', 'train.json')\n",
    "LABEL_DIR_1 = os.path.join('/opt/ml/input/data/', dataset_1, 'ufo', 'train.json')\n",
    "LABEL_DIR_2 = os.path.join('/opt/ml/input/data/', dataset_2, 'ufo', 'train.json')\n",
    "LABEL_DIR_3 = os.path.join('/opt/ml/input/data/', dataset_3, 'ufo', 'train.json')\n",
    "\n",
    "os.makedirs(os.path.join('/opt/ml/input/data/', 'mosaic', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join('/opt/ml/input/data/', 'mosaic', 'ufo'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABEL_DIR_0, 'r', encoding='utf-8') as f:\n",
    "    ufo_0 = json.load(f)['images']\n",
    "ufo_image_names_0 = [x for x in ufo_0.keys()]\n",
    "\n",
    "with open(LABEL_DIR_1, 'r', encoding='utf-8') as f:\n",
    "    ufo_1 = json.load(f)['images']\n",
    "ufo_image_names_1 = [x for x in ufo_1.keys()]\n",
    "\n",
    "with open(LABEL_DIR_2, 'r', encoding='utf-8') as f:\n",
    "    ufo_2 = json.load(f)['images']\n",
    "ufo_image_names_2 = [x for x in ufo_2.keys()]\n",
    "\n",
    "with open(LABEL_DIR_3, 'r', encoding='utf-8') as f:\n",
    "    ufo_3 = json.load(f)['images']\n",
    "ufo_image_names_3 = [x for x in ufo_3.keys()]\n",
    "\n",
    "ufos = [ufo_0, ufo_1, ufo_2, ufo_3]\n",
    "ufo_image_names = [ufo_image_names_0, ufo_image_names_1, ufo_image_names_2, ufo_image_names_3]\n",
    "img_dirs = [IMG_DIR_0, IMG_DIR_1, IMG_DIR_2, IMG_DIR_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img = np.zeros([OUTPUT_SIZE[0], OUTPUT_SIZE[1], 3], dtype=np.uint8)\n",
    "\n",
    "license_tag = dict(usability=True, public=True,\n",
    "                   commercial=True, type='CC-BY-SA',\n",
    "                   holder=None)\n",
    "orientation = 'Horizontal'\n",
    "\n",
    "\n",
    "new_anno = dict()\n",
    "\n",
    "for i in range(TOTAL):\n",
    "    \n",
    "    # scale_x, scale_y는 SCALE_RANGE 사이의 숫자\n",
    "    scale_x = SCALE_RANGE[0] + random.random() * (SCALE_RANGE[1] - SCALE_RANGE[0])\n",
    "    scale_y = SCALE_RANGE[0] + random.random() * (SCALE_RANGE[1] - SCALE_RANGE[0])\n",
    "    divid_point_x = int(scale_x * OUTPUT_SIZE[1])\n",
    "    divid_point_y = int(scale_y * OUTPUT_SIZE[0])\n",
    "\n",
    "    temp_anno = dict(img_h=OUTPUT_SIZE[0], img_w=OUTPUT_SIZE[1],\n",
    "                     tags=None, license_tag=license_tag)\n",
    "    temp_words = dict()\n",
    "    counter = 0\n",
    "    \n",
    "    for j in range(4):\n",
    "        random_index = random.randrange(4)\n",
    "\n",
    "        index = random.randrange(len(ufos[random_index]))\n",
    "        while ((random_index == 0 or random_index == 1) and (index == 102)):\n",
    "            index = random.randrange(len(ufos[random_index]))\n",
    "            \n",
    "        key = ufo_image_names[random_index][index]\n",
    "        value = ufos[random_index][key]\n",
    "        img = cv2.imread(os.path.join(img_dirs[random_index], key))\n",
    "        \n",
    "        try:\n",
    "            img_height, img_width, _ = img.shape\n",
    "        except:\n",
    "            print(\"아래 파일을 살펴보세요~\")\n",
    "            print(os.path.join(img_dirs[random_index], key))\n",
    "            \n",
    "        \n",
    "        # top-left\n",
    "        if j == 0:\n",
    "            try:\n",
    "                img = cv2.resize(img, (divid_point_x, divid_point_y))\n",
    "            except:\n",
    "                print(f\"image shape : {img.shape}\")\n",
    "                print(f\"filename : {cv2.imread(os.path.join(img_dirs[random_index], key))}\")\n",
    "            output_img[:divid_point_y, :divid_point_x, :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "                \n",
    "                a0 = float(int(a0*divid_point_x / img_width))\n",
    "                a1 = float(int(a1*divid_point_y / img_height))\n",
    "                \n",
    "                b0 = float(int(b0*divid_point_x / img_width))\n",
    "                b1 = float(int(b1*divid_point_y / img_height))\n",
    "                \n",
    "                c0 = float(int(c0*divid_point_x / img_width))\n",
    "                c1 = float(int(c1*divid_point_y / img_height))\n",
    "                \n",
    "                d0 = float(int(d0*divid_point_x / img_width))\n",
    "                d1 = float(int(d1*divid_point_y / img_height))\n",
    "\n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        word_tags=word['word_tags'])\n",
    "                counter += 1\n",
    "\n",
    "        # top-right\n",
    "        elif j == 1:\n",
    "            try:\n",
    "                img = cv2.resize(img, (OUTPUT_SIZE[1] - divid_point_x, divid_point_y))\n",
    "            except:\n",
    "                print(f\"image shape : {img.shape}\")\n",
    "                print(f\"filename : {cv2.imread(os.path.join(img_dirs[random_index], key))}\")\n",
    "                \n",
    "            output_img[:divid_point_y, divid_point_x:OUTPUT_SIZE[1], :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "                \n",
    "                a0 = float(int(divid_point_x + (a0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                a1 = float(int(a1*divid_point_y / img_height))\n",
    "                \n",
    "                b0 = float(int(divid_point_x + (b0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                b1 = float(int(b1*divid_point_y / img_height))\n",
    "                \n",
    "                c0 = float(int(divid_point_x + (c0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                c1 = float(int(c1*divid_point_y / img_height))\n",
    "                \n",
    "                d0 = float(int(divid_point_x + (d0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                d1 = float(int(d1*divid_point_y / img_height))\n",
    "                \n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        word_tags=word['word_tags'])\n",
    "                counter += 1\n",
    "\n",
    "        # bottom-left\n",
    "        elif j == 2:\n",
    "            try:\n",
    "                img = cv2.resize(img, (divid_point_x, OUTPUT_SIZE[0] - divid_point_y))\n",
    "            except:\n",
    "                print(f\"image shape : {img.shape}\")\n",
    "                print(f\"filename : {cv2.imread(os.path.join(img_dirs[random_index], key))}\")\n",
    "                \n",
    "            output_img[divid_point_y:OUTPUT_SIZE[0], :divid_point_x, :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "    \n",
    "                a0 = float(int(a0*divid_point_x / img_width))\n",
    "                a1 = float(int(divid_point_y + (a1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                b0 = float(int(b0*divid_point_x / img_width))\n",
    "                b1 = float(int(divid_point_y + (b1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                c0 = float(int(c0*divid_point_x / img_width))\n",
    "                c1 = float(int(divid_point_y + (c1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                d0 = float(int(d0*divid_point_x / img_width))\n",
    "                d1 = float(int(divid_point_y + (d1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "\n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        word_tags=word['word_tags'])\n",
    "                counter += 1\n",
    "\n",
    "        # bottom-right\n",
    "        else:\n",
    "            try:\n",
    "                img = cv2.resize(img, (OUTPUT_SIZE[1] - divid_point_x, OUTPUT_SIZE[0] - divid_point_y))\n",
    "            except:\n",
    "                print(f\"image shape : {img.shape}\")\n",
    "                print(f\"filename : {cv2.imread(os.path.join(img_dirs[random_index], key))}\")\n",
    "            output_img[divid_point_y:OUTPUT_SIZE[0], divid_point_x:OUTPUT_SIZE[1], :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "                \n",
    "                a0 = float(int(divid_point_x + (a0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                a1 = float(int(divid_point_y + (a1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                b0 = float(int(divid_point_x + (b0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                b1 = float(int(divid_point_y + (b1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                c0 = float(int(divid_point_x + (c0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                c1 = float(int(divid_point_y + (c1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                d0 = float(int(divid_point_x + (d0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                d1 = float(int(divid_point_y + (d1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "            \n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        word_tags=word['word_tags'])\n",
    "                counter += 1\n",
    "    \n",
    "    # save image\n",
    "    image_name = f'mosaic_{i}.jpg'\n",
    "    cv2.imwrite(os.path.join('/opt/ml/input/data/', 'mosaic', 'images', image_name), output_img)\n",
    "    \n",
    " \n",
    "    temp_anno['words'] = temp_words\n",
    "    new_anno[image_name] = temp_anno\n",
    "\n",
    "\n",
    "# create ufo train.json\n",
    "anno = dict(images=new_anno)\n",
    "\n",
    "with open(os.path.join('/opt/ml/input/data/', 'mosaic', 'ufo', 'train.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(anno, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모자이크 한 사진 위에 GT 바운딩박스 그려서 보여주기. opt/ml/ 에 생성\n",
    "for word in temp_words.values():\n",
    "    points = np.array(word['points'], dtype=np.int32) \n",
    "    cv2.polylines(output_img, [points], True, (0,255,0), 2)\n",
    "cv2.imwrite('/opt/ml/mosaic.jpg', output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
